{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":451.60874,"end_time":"2022-08-26T18:28:09.055951","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-26T18:20:37.447211","version":"2.3.4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":78209,"databundleVersionId":8563743,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CSC 578 HW\\#7 Intel Image Classification Competition (Spring 2024)\n\n### Name: Art Yalovenko","metadata":{"papermill":{"duration":0.00428,"end_time":"2022-08-26T18:20:45.502864","exception":false,"start_time":"2022-08-26T18:20:45.498584","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dropout\nimport matplotlib.pyplot as plt\nimport csv\nimport pandas as pd\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\n\nimport os\nimport cv2","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.328463,"end_time":"2022-08-26T18:20:50.834888","exception":false,"start_time":"2022-08-26T18:20:45.506425","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-03T16:52:54.865133Z","iopub.execute_input":"2024-06-03T16:52:54.866037Z","iopub.status.idle":"2024-06-03T16:53:07.077952Z","shell.execute_reply.started":"2024-06-03T16:52:54.865991Z","shell.execute_reply":"2024-06-03T16:53:07.077115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the training data. Split into training 80% and validation 20%.","metadata":{"papermill":{"duration":0.003443,"end_time":"2022-08-26T18:20:50.842005","exception":false,"start_time":"2022-08-26T18:20:50.838562","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_directory = '../input/csc-578-hw-7-spring-2024/train'\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(\n    train_directory,\n    labels='inferred',        # use names of subdirectories as target labels\n    label_mode='categorical', # convert target class (int) to one-hot-vector\n    validation_split=0.2,\n    subset=\"training\",\n    seed=123,                 # use same random seed with valid_set\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(150, 150),\n)\n\nvalid_dataset = tf.keras.utils.image_dataset_from_directory(\n    train_directory,\n    labels='inferred',\n    label_mode='categorical',\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(150, 150),\n)","metadata":{"papermill":{"duration":13.392009,"end_time":"2022-08-26T18:21:04.237514","exception":false,"start_time":"2022-08-26T18:20:50.845505","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-03T16:53:07.079697Z","iopub.execute_input":"2024-06-03T16:53:07.080398Z","iopub.status.idle":"2024-06-03T16:53:15.580069Z","shell.execute_reply.started":"2024-06-03T16:53:07.080364Z","shell.execute_reply":"2024-06-03T16:53:15.579254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize some training examples.\nplt.figure(figsize=(10, 12))\nclass_names = train_dataset.class_names\nfor images, labels in train_dataset.take(1):\n    for i in range(30):\n        ax = plt.subplot(6, 5, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[np.argmax(labels[i])])\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:15.581533Z","iopub.execute_input":"2024-06-03T16:53:15.581869Z","iopub.status.idle":"2024-06-03T16:53:18.639231Z","shell.execute_reply.started":"2024-06-03T16:53:15.581843Z","shell.execute_reply":"2024-06-03T16:53:18.638105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Previously Best model (model5)","metadata":{}},{"cell_type":"markdown","source":"## Previously we introduced Learning Scheduler and L2 regularization. Increasing regularization to 0.0025 (tried 0.005 already) and adding a dropout layer","metadata":{}},{"cell_type":"code","source":"# Configure the ReduceLROnPlateau\n\n# monitor= val_loss is pretty self-explanatory, adjust based on plateu in validation loss\n# factor=0.5 is equivalent to learning rate reduction by 1/2 when validation loss stops improving\n# patience = 3 sets that LR will be reduced after 3 consecutive epochs of non-improvement\n# min_lr= 0.00001 lowers eta we will go to is 0.00001\n# verbose = 1 sets to print a message when LR is reduced.\n\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:54:05.238542Z","iopub.execute_input":"2024-06-03T16:54:05.238942Z","iopub.status.idle":"2024-06-03T16:54:05.244004Z","shell.execute_reply.started":"2024-06-03T16:54:05.238912Z","shell.execute_reply":"2024-06-03T16:54:05.243063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model5 = keras.Sequential()\nmodel5.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\nmodel5.add(keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\nmodel5.add(keras.layers.MaxPooling2D(2,2))\nmodel5.add(Dropout(0.25))  # Added Dropout \nmodel5.add(keras.layers.Flatten())\nmodel5.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\nmodel5.add(Dropout(0.25))  # Existing Dropout layer\nmodel5.add(keras.layers.Dense(6, activation='softmax', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\n\n# Compile\nopt = Adam(learning_rate=0.001)  # Using a standard learning rate\nmodel5.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:23:30.088239Z","iopub.execute_input":"2024-06-03T06:23:30.089003Z","iopub.status.idle":"2024-06-03T06:23:30.226314Z","shell.execute_reply.started":"2024-06-03T06:23:30.088969Z","shell.execute_reply":"2024-06-03T06:23:30.225424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history5 = model5.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:23:54.631082Z","iopub.execute_input":"2024-06-03T06:23:54.631706Z","iopub.status.idle":"2024-06-03T06:27:40.848014Z","shell.execute_reply.started":"2024-06-03T06:23:54.631673Z","shell.execute_reply":"2024-06-03T06:27:40.847129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history5.history['accuracy']\nval_acc = history5.history['val_accuracy']\n\nloss = history5.history['loss']\nval_loss = history5.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:27:40.849584Z","iopub.execute_input":"2024-06-03T06:27:40.849905Z","iopub.status.idle":"2024-06-03T06:27:41.316735Z","shell.execute_reply.started":"2024-06-03T06:27:40.849878Z","shell.execute_reply":"2024-06-03T06:27:41.315823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experimenting with batch normalization","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:47.079796Z","iopub.execute_input":"2024-06-03T16:53:47.080155Z","iopub.status.idle":"2024-06-03T16:53:47.084458Z","shell.execute_reply.started":"2024-06-03T16:53:47.080129Z","shell.execute_reply":"2024-06-03T16:53:47.083523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model6 = keras.Sequential()\nmodel6.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\n\n# Convolutional Block with Batch Normalization\nmodel6.add(keras.layers.Conv2D(64, (3,3), use_bias=False))  # Remove bias because BatchNormalization includes a bias component\nmodel6.add(BatchNormalization())\nmodel6.add(keras.layers.Activation('relu'))\nmodel6.add(keras.layers.MaxPooling2D(2,2))\nmodel6.add(Dropout(0.25))  \n\n# Flattening the outputs from the convolutional block to feed into the dense layers\nmodel6.add(keras.layers.Flatten())\n\n# Dense Block with Batch Normalization\nmodel6.add(keras.layers.Dense(128, use_bias=False, kernel_regularizer=l2(0.0025)))  # Remove bias for the same reason\nmodel6.add(BatchNormalization())\nmodel6.add(keras.layers.Activation('relu'))\nmodel6.add(Dropout(0.25))  # Existing Dropout\n\n# Output Layer with Batch Normalization\nmodel6.add(keras.layers.Dense(6, use_bias=False, kernel_regularizer=l2(0.0025)))  # Remove bias\nmodel6.add(BatchNormalization())\nmodel6.add(keras.layers.Activation('softmax'))\n\n# Compile the model \nopt = Adam(learning_rate=0.001)  \nmodel6.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:48.254068Z","iopub.execute_input":"2024-06-03T16:53:48.254742Z","iopub.status.idle":"2024-06-03T16:53:48.440569Z","shell.execute_reply.started":"2024-06-03T16:53:48.254704Z","shell.execute_reply":"2024-06-03T16:53:48.439593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history6 = model6.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:54:08.927516Z","iopub.execute_input":"2024-06-03T16:54:08.927870Z","iopub.status.idle":"2024-06-03T16:58:16.743923Z","shell.execute_reply.started":"2024-06-03T16:54:08.927843Z","shell.execute_reply":"2024-06-03T16:58:16.743090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history6.history['accuracy']\nval_acc = history6.history['val_accuracy']\n\nloss = history6.history['loss']\nval_loss = history6.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:58:57.123322Z","iopub.execute_input":"2024-06-03T16:58:57.124278Z","iopub.status.idle":"2024-06-03T16:58:57.647655Z","shell.execute_reply.started":"2024-06-03T16:58:57.124242Z","shell.execute_reply":"2024-06-03T16:58:57.646738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extremely unstable and overfit** ","metadata":{}},{"cell_type":"markdown","source":"## adding dropout layers after each convolutional and dense layer to provide a more robust regularization effect","metadata":{}},{"cell_type":"code","source":"model7 = keras.Sequential()\nmodel7.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\n\n# Convolutional Block with Batch Normalization and increased Dropout\nmodel7.add(keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.0025), use_bias=False))\nmodel7.add(BatchNormalization())\nmodel7.add(keras.layers.Activation('relu'))\nmodel7.add(keras.layers.MaxPooling2D(2,2))\nmodel7.add(Dropout(0.3))  # Increased dropout rate after MaxPooling\n\n# Additional Convolutional Block for more complex pattern recognition\nmodel7.add(keras.layers.Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.0025), use_bias=False))\nmodel7.add(BatchNormalization())\nmodel7.add(keras.layers.Activation('relu'))\nmodel7.add(keras.layers.MaxPooling2D(2,2))\nmodel7.add(Dropout(0.4))  # Further increased dropout rate after second Convolutional layer\n\n# Flattening the outputs from the convolutional blocks to feed into the dense layers\nmodel7.add(keras.layers.Flatten())\n\n# Dense Block with Batch Normalization and Dropout\nmodel7.add(keras.layers.Dense(128, kernel_regularizer=l2(0.0025), use_bias=False))\nmodel7.add(BatchNormalization())\nmodel7.add(keras.layers.Activation('relu'))\nmodel7.add(Dropout(0.5))  # Increased dropout rate in the dense layer before the output layer\n\n# Output Layer with Batch Normalization\nmodel7.add(keras.layers.Dense(6, use_bias=False, kernel_regularizer=l2(0.0025)))\nmodel7.add(BatchNormalization())\nmodel7.add(keras.layers.Activation('softmax'))\n\n# Compile the model with an optimizer, loss function, and metrics\nopt = Adam(learning_rate=0.001)  # Using a standard learning rate\nmodel7.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:11:59.360444Z","iopub.execute_input":"2024-06-03T17:11:59.361254Z","iopub.status.idle":"2024-06-03T17:11:59.585527Z","shell.execute_reply.started":"2024-06-03T17:11:59.361221Z","shell.execute_reply":"2024-06-03T17:11:59.584727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history7 = model7.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:12:16.672072Z","iopub.execute_input":"2024-06-03T17:12:16.672456Z","iopub.status.idle":"2024-06-03T17:17:32.153249Z","shell.execute_reply.started":"2024-06-03T17:12:16.672426Z","shell.execute_reply":"2024-06-03T17:17:32.152349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**that was fascinating! performance was horrible until the learning rate dropped lower and then it started overfitting??**","metadata":{}},{"cell_type":"code","source":"acc = history7.history['accuracy']\nval_acc = history7.history['val_accuracy']\n\nloss = history7.history['loss']\nval_loss = history7.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:19:39.343523Z","iopub.execute_input":"2024-06-03T17:19:39.343930Z","iopub.status.idle":"2024-06-03T17:19:39.851569Z","shell.execute_reply.started":"2024-06-03T17:19:39.343901Z","shell.execute_reply":"2024-06-03T17:19:39.850617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**While the performance is better across training and validation set, there are signs of overfitting and clear model instability**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We will dial it back to the better performing and simpler model5 and experiment with dropout layers","metadata":{}},{"cell_type":"markdown","source":"### First we increas Dropout after Convolutional Layer","metadata":{}},{"cell_type":"code","source":"model5_1 = keras.Sequential()\nmodel5_1.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\nmodel5_1.add(keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\nmodel5_1.add(keras.layers.MaxPooling2D(2,2))\nmodel5_1.add(Dropout(0.35))  # Added Dropout \nmodel5_1.add(keras.layers.Flatten())\nmodel5_1.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\nmodel5_1.add(Dropout(0.25))  # Existing Dropout layer\nmodel5_1.add(keras.layers.Dense(6, activation='softmax', kernel_regularizer=l2(0.0025)))  # Increased L2 regularization\n\n# Compile\nopt = Adam(learning_rate=0.001)  # Using a standard learning rate\nmodel5_1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:30:18.730718Z","iopub.execute_input":"2024-06-03T17:30:18.731128Z","iopub.status.idle":"2024-06-03T17:30:18.810149Z","shell.execute_reply.started":"2024-06-03T17:30:18.731097Z","shell.execute_reply":"2024-06-03T17:30:18.809329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history5_1 = model5_1.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:30:23.620636Z","iopub.execute_input":"2024-06-03T17:30:23.621007Z","iopub.status.idle":"2024-06-03T17:33:48.006857Z","shell.execute_reply.started":"2024-06-03T17:30:23.620976Z","shell.execute_reply":"2024-06-03T17:33:48.005837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history5_1.history['accuracy']\nval_acc = history5_1.history['val_accuracy']\n\nloss = history5_1.history['loss']\nval_loss = history5_1.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:33:54.679425Z","iopub.execute_input":"2024-06-03T17:33:54.679789Z","iopub.status.idle":"2024-06-03T17:33:55.211407Z","shell.execute_reply.started":"2024-06-03T17:33:54.679748Z","shell.execute_reply":"2024-06-03T17:33:55.210454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Increase only the dropout before the output layer","metadata":{}},{"cell_type":"code","source":"model5_2 = keras.Sequential()\nmodel5_2.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\nmodel5_2.add(keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.0025)))  \nmodel5_2.add(keras.layers.MaxPooling2D(2,2))\nmodel5_2.add(Dropout(0.25))  \nmodel5_2.add(keras.layers.Flatten())\nmodel5_2.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.0025)))  \nmodel5_2.add(Dropout(0.35))  \nmodel5_2.add(keras.layers.Dense(6, activation='softmax', kernel_regularizer=l2(0.0025)))  \n\n# Compile\nopt = Adam(learning_rate=0.001)  # Using a standard learning rate\nmodel5_2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:35:32.000956Z","iopub.execute_input":"2024-06-03T17:35:32.001324Z","iopub.status.idle":"2024-06-03T17:35:32.077542Z","shell.execute_reply.started":"2024-06-03T17:35:32.001298Z","shell.execute_reply":"2024-06-03T17:35:32.076788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history5_2 = model5_2.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:35:34.801184Z","iopub.execute_input":"2024-06-03T17:35:34.801825Z","iopub.status.idle":"2024-06-03T17:39:14.697913Z","shell.execute_reply.started":"2024-06-03T17:35:34.801790Z","shell.execute_reply":"2024-06-03T17:39:14.696996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history5_2.history['accuracy']\nval_acc = history5_2.history['val_accuracy']\n\nloss = history5_2.history['loss']\nval_loss = history5_2.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:39:27.701035Z","iopub.execute_input":"2024-06-03T17:39:27.701622Z","iopub.status.idle":"2024-06-03T17:39:28.258290Z","shell.execute_reply.started":"2024-06-03T17:39:27.701571Z","shell.execute_reply":"2024-06-03T17:39:28.257272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### increase both dropout rates","metadata":{}},{"cell_type":"code","source":"model5_3 = keras.Sequential()\nmodel5_3.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\nmodel5_3.add(keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.0025)))  \nmodel5_3.add(keras.layers.MaxPooling2D(2,2))\nmodel5_3.add(Dropout(0.35))  \nmodel5_3.add(keras.layers.Flatten())\nmodel5_3.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.0025)))  \nmodel5_3.add(Dropout(0.35))  \nmodel5_3.add(keras.layers.Dense(6, activation='softmax', kernel_regularizer=l2(0.0025)))  \n\n# Compile\nopt = Adam(learning_rate=0.001)  # Using a standard learning rate\nmodel5_3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:43:06.426630Z","iopub.execute_input":"2024-06-03T17:43:06.427089Z","iopub.status.idle":"2024-06-03T17:43:06.502079Z","shell.execute_reply.started":"2024-06-03T17:43:06.427057Z","shell.execute_reply":"2024-06-03T17:43:06.501044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history5_3 = model5_3.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:43:07.585634Z","iopub.execute_input":"2024-06-03T17:43:07.586237Z","iopub.status.idle":"2024-06-03T17:46:31.956139Z","shell.execute_reply.started":"2024-06-03T17:43:07.586204Z","shell.execute_reply":"2024-06-03T17:46:31.955341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history5_3.history['accuracy']\nval_acc = history5_3.history['val_accuracy']\n\nloss = history5_3.history['loss']\nval_loss = history5_3.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:55:18.660851Z","iopub.execute_input":"2024-06-03T17:55:18.661741Z","iopub.status.idle":"2024-06-03T17:55:19.190614Z","shell.execute_reply.started":"2024-06-03T17:55:18.661706Z","shell.execute_reply":"2024-06-03T17:55:19.189724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Model5_2 with increased dropout rate befroe the output layer seems to be the best**","metadata":{}},{"cell_type":"markdown","source":"## ** NEW IMPROVED MODEL | No sign of overfitting, stable, and 5% higher accuracy scores across both test and val sets. Much lower loss also | **","metadata":{}},{"cell_type":"markdown","source":"### Lets experiment with filter sizes and convolutional layers to try to capture a little bit more complexity since we seemingly have enough regularization\n\n#### also decreased learning rate to 0.0001 and regularization from 0.0025 to 0.001","metadata":{}},{"cell_type":"code","source":"model5_x = keras.Sequential()\nmodel5_x.add(keras.layers.Rescaling(1./255, input_shape=(150, 150, 3)))\n\n# First convolutional layer with smaller filters to capture fine details\nmodel5_x.add(keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.001)))\nmodel5_x.add(keras.layers.MaxPooling2D(2,2))\nmodel5_x.add(Dropout(0.25))\n\n# Second convolutional layer with larger filters to capture broader features\nmodel5_x.add(keras.layers.Conv2D(64, (5,5), activation='relu', kernel_regularizer=l2(0.001)))\nmodel5_x.add(keras.layers.MaxPooling2D(2,2))\nmodel5_x.add(Dropout(0.25))\n\n# Added a third layer to enhance feature capture\nmodel5_x.add(keras.layers.Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.001)))\nmodel5_x.add(keras.layers.MaxPooling2D(2,2))\nmodel5_x.add(Dropout(0.35))\n\nmodel5_x.add(keras.layers.Flatten())\nmodel5_x.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\nmodel5_x.add(Dropout(0.35))\nmodel5_x.add(keras.layers.Dense(6, activation='softmax', kernel_regularizer=l2(0.001)))\n\n# Compile the model\nopt = Adam(learning_rate=0.0001)\nmodel5_x.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:47:43.791064Z","iopub.execute_input":"2024-06-03T18:47:43.791440Z","iopub.status.idle":"2024-06-03T18:47:43.935604Z","shell.execute_reply.started":"2024-06-03T18:47:43.791410Z","shell.execute_reply":"2024-06-03T18:47:43.934822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history5_x = model5_x.fit(\n    train_dataset, \n    epochs=15, \n    validation_data=valid_dataset, \n    callbacks=[lr_scheduler] \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:47:46.886362Z","iopub.execute_input":"2024-06-03T18:47:46.886721Z","iopub.status.idle":"2024-06-03T18:50:40.102522Z","shell.execute_reply.started":"2024-06-03T18:47:46.886691Z","shell.execute_reply":"2024-06-03T18:50:40.101655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history5_x.history['accuracy']\nval_acc = history5_x.history['val_accuracy']\n\nloss = history5_x.history['loss']\nval_loss = history5_x.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:40:43.070874Z","iopub.execute_input":"2024-06-03T18:40:43.071230Z","iopub.status.idle":"2024-06-03T18:40:43.623988Z","shell.execute_reply.started":"2024-06-03T18:40:43.071204Z","shell.execute_reply":"2024-06-03T18:40:43.622985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run model on Test Set","metadata":{}},{"cell_type":"code","source":"# Load the test_pred data (which has no target labels)\n\npred_directory = '../input/csc-578-hw-7-spring-2024/test_pred'\nresult_dict = {} # dictionary to store predictions (keyed by file number)\n\n# iterate over files in that directory\nfor filename in os.listdir(pred_directory):\n    f = os.path.join(pred_directory, filename)\n    # checking if it is a file\n    if os.path.isfile(f):\n        fnum = int(filename[:-4]) # filename e.g. '103.jpg\" -> 103\n        img = cv2.imread(f)\n        #img = img/255.0\n        img = img.reshape(-1,150,150,3)\n        pred = model5_x.predict(img)\n        result_dict[fnum] = pred[0]  # [0] because there is only one data\nprint (len(result_dict))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:54:42.760697Z","iopub.execute_input":"2024-06-03T18:54:42.761074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the results by file number\nsorted_results = sorted(result_dict.items())\n\n# start a new CSV file\nwith open('submission.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    # add the header\n    writer.writerow(['fnum', 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'])\n    # Write in the predictions\n    for fnum, probs in sorted_results:\n        writer.writerow([fnum] + list(probs))\n\nprint(\"'submission.csv' created successfully!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:42:20.259728Z","iopub.status.idle":"2024-06-03T05:42:20.260263Z","shell.execute_reply.started":"2024-06-03T05:42:20.259972Z","shell.execute_reply":"2024-06-03T05:42:20.259994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}